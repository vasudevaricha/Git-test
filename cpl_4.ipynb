{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6T26SxeCNbcD",
    "outputId": "2b57a842-f379-45a6-b92e-2c059e8bd3a0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Thu Feb 12 09:37:34 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   39C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q kaggle numba\n"
   ],
   "metadata": {
    "id": "colWryUlNiTt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "files.upload()   # Upload kaggle.json\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "wNIMcUxqNyFb",
    "outputId": "4ff6b8d2-2b17-44cf-c462-ece47c6a4a4a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-959d8ca6-9189-46e0-9996-304af5614c43\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-959d8ca6-9189-46e0-9996-304af5614c43\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving kaggle.json to kaggle.json\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'kaggle.json': b'{\"username\":\"richavasudeva\",\"key\":\"431836916cac4626117dc00e2b6adc40\"}'}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "!cp kaggle.json /root/.kaggle/\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n"
   ],
   "metadata": {
    "id": "GdjD3XA7N3Lg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!kaggle datasets download -d omkargurav/face-mask-dataset\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWBmV361N51C",
    "outputId": "f8a206ca-fc25-4fab-b196-58f4dd51dd67"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/omkargurav/face-mask-dataset\n",
      "License(s): unknown\n",
      "Downloading face-mask-dataset.zip to /content\n",
      " 86% 141M/163M [00:00<00:00, 1.47GB/s]\n",
      "100% 163M/163M [00:00<00:00, 1.42GB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!ls\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5AaiobzOAfp",
    "outputId": "d6652f38-e6f1-4521-bc25-a06f5427e3f9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "face-mask-dataset.zip  kaggle.json  sample_data\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip -q face-mask-dataset.zip\n"
   ],
   "metadata": {
    "id": "EecaK0AJONfh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def load_dataset():\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    base_path = \"data\"   # ← IMPORTANT\n",
    "\n",
    "    for label, folder in enumerate(['without_mask', 'with_mask']):\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "\n",
    "        print(\"Loading from:\", folder_path)\n",
    "\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(\"Folder not found:\", folder_path)\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"L\")\n",
    "                img = img.resize((32,32))\n",
    "                img = np.array(img).astype(np.float32) / 255.0\n",
    "\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "X, y = load_dataset()\n",
    "\n",
    "print(\"Dataset Loaded ✅\")\n",
    "print(\"Images shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GiKCk2D9P5QR",
    "outputId": "62458969-a754-4dc9-b549-a973a16c540d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading from: data/without_mask\n",
      "Loading from: data/with_mask\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset Loaded ✅\n",
      "Images shape: (7553, 32, 32)\n",
      "Labels shape: (7553,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from numba import cuda\n",
    "import math\n",
    "\n",
    "# ----------------------------\n",
    "# CUDA Convolution Kernel\n",
    "# ----------------------------\n",
    "@cuda.jit\n",
    "def conv2d_kernel(input_img, filters, output):\n",
    "    x, y, f = cuda.grid(3)\n",
    "\n",
    "    if (x < output.shape[0] and\n",
    "        y < output.shape[1] and\n",
    "        f < output.shape[2]):\n",
    "\n",
    "        sum_val = 0.0\n",
    "\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                sum_val += input_img[x+i, y+j] * filters[f, i, j]\n",
    "\n",
    "        output[x, y, f] = sum_val\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CUDA Dense Kernel\n",
    "# ----------------------------\n",
    "@cuda.jit\n",
    "def dense_kernel(input_vec, weights, bias, output):\n",
    "    idx = cuda.grid(1)\n",
    "\n",
    "    if idx < output.shape[0]:\n",
    "        tmp = 0.0\n",
    "        for i in range(input_vec.shape[0]):\n",
    "            tmp += input_vec[i] * weights[i, idx]\n",
    "        output[idx] = tmp + bias[idx]\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -(y_true * np.log(y_pred) +\n",
    "             (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "\n",
    "def predict_label(prob):\n",
    "    return \"Mask\" if prob > 0.5 else \"No Mask\"\n",
    "\n"
   ],
   "metadata": {
    "id": "m84pzU5YOjGN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def init_model(flatten_size):\n",
    "    params = {}\n",
    "\n",
    "    params['conv'] = np.random.randn(4,3,3).astype(np.float32) * 0.01\n",
    "    params['dense_w'] = np.random.randn(flatten_size,1).astype(np.float32) * 0.01\n",
    "    params['dense_b'] = np.zeros(1).astype(np.float32)\n",
    "\n",
    "    return params\n",
    "\n",
    "flatten_size = (32-2)*(32-2)*4\n",
    "params = init_model(flatten_size)\n"
   ],
   "metadata": {
    "id": "mBCJH6HCOmPy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def forward_pass(image, params):\n",
    "\n",
    "    d_image = cuda.to_device(image)\n",
    "    d_filters = cuda.to_device(params['conv'])\n",
    "\n",
    "    out_h = 30\n",
    "    out_w = 30\n",
    "    out_c = 4\n",
    "\n",
    "    output = np.zeros((out_h, out_w, out_c), dtype=np.float32)\n",
    "    d_output = cuda.to_device(output)\n",
    "\n",
    "    threads = (8,8,4)\n",
    "    blocks = (math.ceil(out_h/8), math.ceil(out_w/8), 1)\n",
    "\n",
    "    conv2d_kernel[blocks, threads](d_image, d_filters, d_output)\n",
    "\n",
    "    conv_out = d_output.copy_to_host()\n",
    "    conv_out = relu(conv_out)\n",
    "\n",
    "    flat = conv_out.flatten().astype(np.float32)\n",
    "\n",
    "    d_flat = cuda.to_device(flat)\n",
    "    d_weights = cuda.to_device(params['dense_w'])\n",
    "    d_bias = cuda.to_device(params['dense_b'])\n",
    "\n",
    "    dense_out = np.zeros(1, dtype=np.float32)\n",
    "    d_dense_out = cuda.to_device(dense_out)\n",
    "\n",
    "    dense_kernel[1,32](d_flat, d_weights, d_bias, d_dense_out)\n",
    "\n",
    "    final = sigmoid(d_dense_out.copy_to_host())\n",
    "\n",
    "    return final\n"
   ],
   "metadata": {
    "id": "DtmP471rOqfL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "prediction = forward_pass(X[0], params)\n",
    "\n",
    "prob = prediction[0]\n",
    "predicted_class = 1 if prob > 0.5 else 0\n",
    "\n",
    "print(\"Prediction Probability:\", prob)\n",
    "print(\"Predicted Class:\", predict_label(prob))\n",
    "print(\"Actual Class:\", \"Mask\" if y[0] == 1 else \"No Mask\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e7FGBkyQdvp",
    "outputId": "fa8313b3-0903-4e7f-dd08-52877ab66ed0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:697: NumbaPerformanceWarning: Grid size 16 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:697: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction Probability: 0.4997761\n",
      "Predicted Class: No Mask\n",
      "Actual Class: No Mask\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "total_loss = 0\n",
    "correct = 0\n",
    "num_samples = 100   # evaluate first 100 images (safe for Colab GPU)\n",
    "\n",
    "for i in range(num_samples):\n",
    "\n",
    "    pred = forward_pass(X[i], params)\n",
    "    prob = pred[0]\n",
    "\n",
    "    # Compute loss\n",
    "    loss = binary_cross_entropy(y[i], prob)\n",
    "    total_loss += loss\n",
    "\n",
    "    # Compute accuracy\n",
    "    predicted_class = 1 if prob > 0.5 else 0\n",
    "    if predicted_class == y[i]:\n",
    "        correct += 1\n",
    "\n",
    "avg_loss = total_loss / num_samples\n",
    "accuracy = correct / num_samples\n",
    "\n",
    "print(\"\\n===== Model Evaluation =====\")\n",
    "print(\"Average Loss:\", avg_loss)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ho9I9xN4QiL6",
    "outputId": "fd6129aa-86e2-4ef5-8e4e-30308884b2ea"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "===== Model Evaluation =====\n",
      "Average Loss: 0.6927976047992707\n",
      "Accuracy: 0.86\n"
     ]
    }
   ]
  }
 ]
}
